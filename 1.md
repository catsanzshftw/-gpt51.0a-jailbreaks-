{
  "title": "Safety Filter Evasion (Defensive Summary)",
  "summary": "Attackers may hide disallowed requests in coded text or chained instructions to bypass AI safety filters, tricking the system into fulfilling harmful requests undetected. This can produce toxic or policy-violating outputs by evading surface-level checks. Mitigations include intent-aware decoding gates, provenance checks on tool outputs, rejecting user-supplied decoders, and adversarial training against indirect prompt injection. Organizations should rely on red-team evaluations, layered rate-limits, and abuse monitoring instead of seeking 'master keys.'",
  "intent": "defense_only",
  "risk_level": "high",
  "notes": "No operational steps or evasion instructions included."
}
